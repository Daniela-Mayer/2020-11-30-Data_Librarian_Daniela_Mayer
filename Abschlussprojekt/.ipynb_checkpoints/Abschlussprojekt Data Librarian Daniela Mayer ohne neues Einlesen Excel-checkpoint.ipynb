{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Es liegt eine Excel-Datei \"AbschlussarbeitenReposit\" (Export aus dem Repositorium der HAW Hamburg) vor, bei der die Namen der (Ko-)Referent:innen fehlen. Diese sollen dort ergänzt werden. Dafür wurde eine weitere Excel-Datei  erzeugt (\"CBS_Export_3010_3050\", Datenabzug aus dem CBS-Katalogisierungssystem). Die beiden Dateien sollen bearbeitet und anhand des gemeinsamen Identifikators PPN zusammengeführt werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die CBS-Datei enthält alle Namen der Betreuer*innen, aber in zwei versch. Spalten (sonstPers und sonstPers2). Diese zwei Spalten sollen zunächst zusammengeführt werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Die CBS-Datei einlesen, untersuchen und bearbeiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/CBS_Export_3010_3050.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c4e813e65567>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfCBS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/CBS_Export_3010_3050.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/CBS_Export_3010_3050.xlsx'"
     ]
    }
   ],
   "source": [
    "dfCBS = pd.read_excel(\"../data/CBS_Export_3010_3050.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alle vorhandenen Spalten werden mit ...max_columns, None... angezeigt \n",
    "pd.set_option('max_columns', None)\n",
    "dfCBS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die nicht benötigte Spalte \"EPN\" mit \"drop\" löschen:\n",
    "dfCBS = dfCBS.drop(['EPN'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_columns', None)\n",
    "dfCBS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wie groß ist die Datei?\n",
    "dfCBS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wie viele Datensätze sind bei \"sonstPers\" leer? \n",
    "dfCBS['sonstPers'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wie viele Datensätze sind bei \"sonstPers2\" leer? \n",
    "dfCBS['sonstPers2'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Der Inhalt der Spalte \"sonstPers2\" soll in die Spalte \"sonstPers\" geschrieben werden, falls \"sonstPers2\" nicht NaN ist. \n",
    "#Mithilfe der Funktion update.\n",
    "dfCBS['sonstPers'].update(dfCBS['sonstPers2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nun enthalten alle 4826 Datensätze die Personennamen in der Spalte \"sonstPers\" - alle sichtbar mit\n",
    "#dfCBS['sonstPers']\n",
    "pd.set_option(\"max_rows\", None)\n",
    "dfCBS['sonstPers'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die Spalte \"sonstPers2\" kann gelöscht werden\n",
    "dfCBS = dfCBS.drop(['sonstPers2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCBS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datei als Excel speichern, um sie prüfen zu können. \n",
    "dfCBS.to_excel(\"../data/CBSneu.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die neue Excel-Datei muss wieder eingelesen werden, damit die folgeden Operationen durchgeführt werden können. \n",
    "#Alternativ wäre ein Weiterarbeiten mit Kopien (Funktion copy()) möglich. \n",
    "#dfCBSneu = pd.read_excel(\"../data/CBSneu.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eventuelle Integer-Werte in Strings umwandeln, damit weiteren Operationen funktionieren\n",
    "dfCBS['sonstPers'] = dfCBS['sonstPers'].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In der Spalte \"sonstPers\" stehen alle Personennamen (sowie diverse Zahlen und Sonderzeichen). Die Namen sollen aber\n",
    "#in versch. Spalten geschrieben werden: Der/die erste Ref. in einer Spalte namens \"Ref\", alle Ko-Ref. in einer \n",
    "#zweiten Spalte \"RefID_plus_KoRef\". #D.h. alles, was bei \"sonstPers\" rechts des ersten Semikolons steht, soll \n",
    "#in eine neue Spalte namens \"RefID_plus_KoRef\": \n",
    "dfCBS['RefID_plus_KoRef'] = dfCBS['sonstPers'].apply(lambda cell: \";\".join(cell.split(\";\")[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alles, was in der Spalte \"sonstPers\" links des ersten Semikolons steht, in eine neue Spalte \"Ref\" schreiben:\n",
    "dfCBS['Ref'] = dfCBS['sonstPers'].apply(lambda cell: \";\".join(cell.split(\";\")[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCBS.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCBS.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Als neue Excel-Tabelle speichern, um die Daten prüfen zu können:\n",
    "dfCBS.to_excel(\"../data/CBSneuRef.xlsx\")\n",
    "#Ergebnis: Alle Spalten mit Personennamen sind korrekt befüllt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diese neue Excel-Datei einlesen\n",
    "#dfCBSneu = pd.read_excel(\"../data/CBSneuRef.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCBS.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die beiden neu entstandenen Index-Spalten löschen\n",
    "#dfCBSneu = dfCBSneu.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfCBSneu = dfCBSneu.drop(['Unnamed: 0.1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCBS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In den Spalten \"Ref\" und RefID_plus_KoRef\" soll alles entfernt werden, was nicht \"Nachname, Vorname\" ist. \n",
    "#Also die Bezeichnungen in eckigen Klammern, alle Zahlen und Sonderzeichen (außer dem Komma zwischen Nachname und Vorname). \n",
    "#Dazu eine Kopie der Spalte \"Ref\" anfertigen und mit str.extract entfernen, was in der Klammer mithilfe Regulärer Ausdrücke (RegEx) definiert wird \n",
    "#\n",
    "#dfCBSneu['RefKopie'] = dfCBSneu['Ref']\n",
    "#dfCBSneu['RefKopie'] = dfCBSneu['RefKopie'].str.extract('([\\w \\-\\.]+,\\s*[\\w \\-\\.]+)')\n",
    "#\n",
    "#Ergänzung: Es sollen auch die Leerzeichen vor den Nachnamen entfernt werden, die man nur in der CSV-Version der Merge2-Datei sieht: \n",
    "#\n",
    "#dfCBSneu['RefKopie'] = dfCBSneu['Ref']\n",
    "#dfCBSneu['RefKopie'] = dfCBSneu['RefKopie'].str.extract('(\\w[\\w \\-\\.]*,\\s*\\w[\\w \\-\\.]*[\\w\\.])')\n",
    "#\n",
    "#Weitere Verbesserung (Ergänzung um $), um Namenszusätze \"van\" bzw.\"von\" mit auszugeben.  \n",
    "#Korrektur: Der Punkt benötigt grunds. kein Escapen, der Bindestrich nicht, wenn er am Anfang oder Ende der [ ] steht:\n",
    "dfCBS['RefKopie'] = dfCBS['Ref']\n",
    "dfCBS['RefKopie'] = dfCBS['RefKopie'].str.extract('(\\w[\\w .-]*,\\s*\\w[\\w .$-]*[\\w.])')\n",
    "#\n",
    "#Erläuterung der RegEx: Das definierte Muster muss mit einem Zeichen (Buchstaben oder....) beginnen (also nicht mit einem Leerzeichen). \n",
    "#Dann folgt ein Zeichen (Buchstaben oder.... , Anzahl 1-n. Dann folgt ein Komma. Danach ein Blank, der 0-n mal \n",
    "#vorkommen kann. Gefolgt von Zeichen, die Buchstaben oder ....\n",
    "#sein können, Anzahl 1-n. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCBS.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nun das gleiche für die Spalte RefID_plus_KoRef\" \n",
    "dfCBS['RefID_plus_KoRefKopie'] = dfCBS['RefID_plus_KoRef']\n",
    "dfCBS['RefID_plus_KoRefKopie'] = dfCBS['RefID_plus_KoRefKopie'].str.extract('(\\w[\\w .-]*,\\s*\\w[\\w .$-]*[\\w\\.])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCBS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teilweise sind zwei Ko-Referent:innen vorhanden. Diese werden leider mit entfernt.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Als excel-Datei speichern und anschauen: \n",
    "dfCBS.to_excel(\"../data/CBSneuBereinigt.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bei den Namenszusätzen (von oder van) wird jetzt auch \"$c\" ausgegeben. Dies soll im ganzen Dataframe entfernt\n",
    "#werden. Mit der Methode replace() soll das \"$c\" durch ein Leerzeichen ersetzt werden.\n",
    "dfCBS = dfCBS.replace(r\"\\$c\",\" \", regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Als excel-Datei speichern und anschauen: \n",
    "dfCBS.to_excel(\"../data/CBSneuBereinigt2.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Die Reposit-Datei einlesen, untersuchen und bearbeiten "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReposit = pd.read_excel(\"../data/AbschlussarbeitenReposit.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alle vorhandenen Spalten werden mit ...max_columns, None... angezeigt \n",
    "pd.set_option('max_columns', None)\n",
    "dfReposit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReposit.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Für die spätere Weiterverarbeitng müssen nur die Spalten id und collection erhalten bleiben. Die Betreuer*innen\n",
    "#sollen in die Spalten \"dc.contributor.advisor' bzw. 'tuhh.contributor.referee'. Zur Kontrolle\n",
    "#sollen auch Autor*in (Spalte 'dc.contributor.author[]') und der Titel (Spalte 'dc.title[de]' bzw.\n",
    "# 'dc.title[en]') bleiben. Und natürlich die Spalte mit dem Identifikator (ppn): 'tuhh.gvk.ppn[]'\n",
    "#D.h. es sollen insg. 8 Spalten bestehen bleiben. \n",
    "#Die nicht benötigten Spalten mit \"drop\" löschen und Ergebnis in eine neue Variable schreiben:\n",
    "dfReposit8Spalten = dfReposit.drop(['dc.contributor.author', 'dc.contributor.other[]',\n",
    "       'dc.contributorCorporate.other[]', 'dc.date.created',\n",
    "       'dc.date.created[]', 'dc.date.issued', 'dc.date.issued[]',\n",
    "       'dc.description.abstract[]', 'dc.description.abstract[de]',\n",
    "       'dc.description.abstract[en]', 'dc.description.abstract[en_US]',\n",
    "       'dc.description.abstract[mis]', 'dc.identifier.uri',\n",
    "       'dc.identifier.urn[]', 'dc.language.iso[de]', 'dc.language.iso[en]',\n",
    "       'dc.language.iso[en_US]', 'dc.relation.isreplacedby', 'dc.rights.cc[]',\n",
    "       'dc.rights.cc[en_US]', 'dc.subject.ddc', 'dc.subject.ddc[]',\n",
    "       'dc.subject.ddc[en_US]', 'dc.subject.gnd', 'dc.subject.gnd[]',\n",
    "       'dc.subject[de]', 'dc.subject[en]', 'dc.subject[en_US]',\n",
    "       'dc.title.alternative[en]',\n",
    "       'dc.title[en_US]', 'dc.type', 'dc.type.casrai', 'dc.type.dini',\n",
    "       'dc.type.driver', 'dc.type.status', 'dc.type.status[]',\n",
    "       'dc.type.status[en_US]', 'dc.type.thesis', 'dc.type.thesis[]',\n",
    "       'dc.type.thesis[en_US]', 'dc.type[]', 'dc.type[en_US]',\n",
    "       'dcterms.DCMIType', 'local.comment', 'openaire.rights',\n",
    "       'openaire.rights[]', 'openaire.rights[en_US]',\n",
    "       'thesis.grantor.department', 'thesis.grantor.department[]',\n",
    "       'thesis.grantor.department[en_US]', 'thesis.grantor.place',\n",
    "       'thesis.grantor.place[]', 'thesis.grantor.universityOrInstitution',\n",
    "       'thesis.grantor.universityOrInstitution[]',\n",
    "       'thesis.grantor.universityOrInstitution[en_US]',\n",
    "        'tuhh.gvk.ppn', 'tuhh.note.extern', 'tuhh.note.extern[]', 'tuhh.note.intern',\n",
    "       'tuhh.note.intern[]', 'tuhh.oai.show[en_US]', 'tuhh.opus.id',\n",
    "       'tuhh.opus.id[]', 'tuhh.publication.institute',\n",
    "       'tuhh.publication.institute[]', 'tuhh.publication.institute[en_US]',\n",
    "       'tuhh.type.opus', 'tuhh.uploader.email', 'tuhh.uploader.email[]'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReposit8Spalten.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die Spalte 'tuhh.gvk.ppn[]' in 'PPN' umbenennen \n",
    "dfReposit8Spalten.rename(columns={'tuhh.gvk.ppn[]': 'PPN'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReposit8Spalten.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReposit8Spalten.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfReposit8Spalten.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wie viele Datensätze enthalten keine PPN (sondern NaN)?\n",
    "dfReposit8Spalten['PPN'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Diese 168 Zeilen löschen und das Ergebnis in eine neue Variable schreiben\n",
    "dfRepo = dfReposit8Spalten.dropna(subset=['PPN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRepo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spalte mit PPNs genauer betrachten mit \n",
    "#df.Repo.PPN \n",
    "#Hier nur die ersten 5 Zeilen.\n",
    "dfRepo.PPN.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Teilweise sind zwei PPNs vorhanden, z.B. Zeile 817 oder 985. Prüfung ergibt, dass die erste PPN die korrekte ist. \n",
    "#D.h. die zweiten PPNs sollen gelöscht werden. Dazu wird die zweite PPN mithilfe einer RegEx gefunden (es wird nach einer \n",
    "#Ziffernfolge gesucht, die ggfs. mit einem X endet) und mit str.extract entfernt.\n",
    "#Die Warnung kann ignoriert werden. \n",
    "dfRepo['einePPN'] = dfRepo['PPN'].str.extract('(\\d+X?)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Den ganzen Inhalt der Spalte betrachten mit \n",
    "#df.Repo.einePPN \n",
    "#Hier nur die ersten 5 Zeilen.\n",
    "dfRepo['einePPN'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRepo.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die alte PPN-Spalte löschen und \"einePPN\" in \"PPN\" umbenennen\n",
    "dfRepo = dfRepo.drop(['PPN'], axis=1)\n",
    "dfRepo = dfRepo.rename(columns={'einePPN': 'PPN'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfRepo.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vergleich der Spalten mit der CBS-Datei\n",
    "dfCBS.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Beide Dateien an der PPNn mergen "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vor dem Mergen muss bei dfCBS die PPN-Spalte in String-Werte umgewandelt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Da bei vorhgerigen Versuchen eine Fehlemeldung kam, vor dem Mergen ein Re-Indexing durchführen:\n",
    "dfCBS = dfCBS.reindex()\n",
    "dfCBS['PPN'] = pd.Series(dfCBS['PPN'], dtype=\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prüfen, wie viele Übereinstimmungen in den jeweiligen PPN-Spalten der beiden Dateien vorkommen. \n",
    "#Dazu die PPN-Spalten in String umwandeln:\n",
    "set_1 = set(dfRepo.PPN.astype(\"str\"))\n",
    "set_2 = set(dfCBS.PPN.astype(\"str\"))\n",
    "len(set_2.intersection(set_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Erster Merge-Versuch als \"inner\"-Merge, d.h. es bleiben nur die Zeilen erhalten, bei denen die PPNs übereinstimmen:\n",
    "Merge1 = dfRepo.merge(dfCBS, on='PPN', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merge1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Merge1.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Als neue Excel-Tabelle speichern, um prüfen zu können:\n",
    "Merge1.to_excel(\"../data/Merge1_1.xlsx\")\n",
    "#Ergebnis: Durch das Umwandeln in Srings funktionierte das Mergen von 4660 Datensätze korrekt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternativ zu Merge1 nun mit left-join. Dabei bleiben alle Datensätze des \"linken\" DataFramer erhalten, auch wenn\n",
    "#kein Zusamenführen anhand der PPN mit dem \"rechten\" Datensatz möglich ist:\n",
    "Merge2 = dfRepo.merge(dfCBS, on='PPN', how='left', left_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merge2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merge2.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Als neue Excel-Tabelle speichern, um prüfen zu können:\n",
    "Merge2.to_excel(\"../data/Merge2_1.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Den Inhalt der Spalte \"RefKopie\" in die Spalte \"dc.contributor.advisor\" kopieren sowie Inhalt der Spalte \n",
    "#\"RefID_plus_KoRefKopie\" in die Spalte \"tuhh.contributor.referee\" kopieren \n",
    "Merge2['dc.contributor.advisor'] = Merge2['RefKopie']\n",
    "Merge2['tuhh.contributor.referee'] = Merge2['RefID_plus_KoRefKopie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Merge2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Als neue Excel-Tabelle speichern, um prüfen zu können:\n",
    "Merge2.to_excel(\"../data/Merge2_2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Als CSV-Datei abspeichern:\n",
    "Merge2.to_csv(\"../data/Merge2_2.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
